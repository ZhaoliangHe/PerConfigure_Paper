\section{Related Works}
\label{Section: related_works}

%\subsection{Configuration optimization} 
\subsection{Static configuration optimization}
Several previous papers have considered optimizing video analytics servicess by either adjusting the configuration knobs or training specialized NN models. VideoStorm \cite{zhang2017videostorm} profiles thousands of video analytics queries on live video streams over large clusters, achieving resource-quality tradeoff with multi-dimensional configurations. VideoEdge \cite{hung2018videoedge} introduces \emph{dominant demand} to identify the best tradeoff between multiple resources and accuracy, and narrows the search space by identifying a ``Pareto ban'' of promising configurations. MCDNN \cite{han2016mcdnn} provides a heuristic scheduling algorithm to adaptively select model variants of different accuracy for deep stream processing under resource constraints. Focus \cite{hsieh2018focus} deconstructs video analytics into two phases, i.e., video ingest and video query. By tuning the share of computing resources of both phases, Focus achieves effective and flexible tradeoff of latency and accuracy of video analytics. These algorithms all profile and optimize video analytics only once at the beginning of the video. In their works, video is divided into pictures at a constant frame rate, and the work of video analyzing is regarded as a fixed task either. In other words, they do not handle changes in video stream context. But the optimal configurations do change over time because of the complex and changeable video stream contexts.

\subsection{Dynamic configuration optimization}
Some papers study how to dynamically optimize the configuration for video analytics when the video stream context changes. \cite{shen2017retrain_model} adaptively retrains the NN model to detect the set of popular objects as it changes over time in the video classification task. \cite{yang2019edge_coordinated} proposes an online video quality and computing resource configuration algorithm to gradually learn the optimal configuration strategy, effectively improving the analytic accuracy while providing the low-latency response. INFaaS \cite{romero2019infaas} automatically selects a model, hardware architecture, and any compiler optimizations, and makes scaling and resource allocation decisions when application load varies and the available resources vary over time. QuickAdapt \cite{ullah2019quickadapt} uses
descriptive statistics of security events data and fuzzy rules to
enable a Big Data Cyber Security Analytics (BDCA) system to quickly adapt to the changes in security events data. JCAB \cite{wang2020jcab} jointly optimizes configuration adaption and bandwidth allocation to address several critical challenges in edge-based video analytics systems, including edge capacity limitation, unknown network variation, intrusive dynamics of video contexts. The online algorithm effectively balances analytics accuracy and energy consumption while keeping low system latency. \cite{argerich2019orchestration} leverages tabular Q-learning to adapt configuration, but their agent's states only consider last latency. Our framework pays attention to the complex and changeable video stream contexts, and picks the best configuration according to current video context. 

The closest work to ours is Chameleon \cite{jiang2018chameleon}, which dynamically picks the best configurations for video analytics servicess, reducing resource consumption with little degradation in accuracy. They leverage temporal and spatial correlation to amortizes the cost of profiling over time and across multiple cameras, and exploit the knob independence to reduce the search space from exponential to linear.\cite{jiang2018chameleon} Notice that Chameleon still has non-trivial room for improvement compared to the optimal (idealized) performance, i.e.,
periodic updating with zero profiling cost. Even the search space is linear, and the profiling cost is still expensive. Such a 24-hours video, Chameleon profiles the configuration space once in every profiling window (16s), it would profile 5400 times. One profiling cost grows linear in the number of configuration knobs and the number of values per knob. The total profiling cost is also significantly high, which is equal to one profiling cost multiply the number of profiling (5400). Our solution, called AutoConfigure, leverages a reinforcement learning-agent to automatically choose the best configuration periodically, significantly reducing the cost of profiling since the agent's choosing time is extremely lower. 

