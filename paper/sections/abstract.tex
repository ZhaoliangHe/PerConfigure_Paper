\begin{abstract}
Deep convolutional neural networks (NN)-based video analytics services demand intensive computation resources and high inference accuracy. The static configuration would either waste computation resources (by picking an expensive configuration) or decrease inference accuracy. Knowing this, adapting video configuration is still extremely challenging: 1) The best video configuration is determined by confounding factors, including the characteristics of the input video, the various accuracy-demand services, and computation resource, etc. 2) The cost of adaptive configuration profiling may far exceed the benefits of adaptive configurations. To tackle these challenges, we propose a reinforcement learning-based video analytics configuration framework, AdaConfigure. In particular, we design an agent that adaptively chooses the configuration according to the spatial and temporal features of video contexts. We design a reward with comprehensive considering of accuracy and computation resources to assess each configuration's impact, guiding the agent learning to choose the best configuration for different accuracy-demand services. We leverage dividing video strategy and the extremely short choosing action time of agent to reduce profiling cost, which is only 0.1-1\% overhead to the overall video analytics services. Our evaluation experiments using the object detection task show that our approach outperforms static configurations by achieves 10-35\% higher accuracy with a similar amount of computation resources or achieves similar accuracy with only 60-90\% of the computation resources.
\end{abstract}

%2) It is hard to assess the impact that each configuration change will produce. 

%Deep convolutional neural networks (NN)-based video analytics services demand intensive computation resources and high inference accuracy. Due to the highly variable video context, the \emph{best} configuration also varies over time. If one chooses a static configuration (i.e., only profiles the video stream to choose the best configuration \emph{once}), the services would either waste computation resources (by picking an expensive configuration) or decrease inference accuracy. Conversely, searching for all configurations in an enormous space can lead to excessive resource overhead that far exceeds the benefits of periodic configurations. Knowing this, designing an \emph{adaptive} approach to choose the best configuration for the current video context is meaningful. We propose a reinforcement learning (RL)-based adaptive video analytics configuration framework, AutoConfigure. The unique feature of AutoConfigure is \emph{context-driven}, meaning that AutoConfigure can adapt the best configuration to intrusive dynamics of video contexts. In particular, our solution can choose the best configuration for the current video chunk according to the spatial and temporal features of video contexts. We implement and evaluate this approach in the object detection task, comparing its performance to static configuration. We show that AutoConfigure achieves 10-35\% higher accuracy with a similar amount of computation resources or achieves similar accuracy with only 60-90\% of the computation resources. Our solution proves to be more efficient than static solutions and only creates an overhead of 0.1-1\% to the overall video analytics services.
%and dynamic configuration baseline