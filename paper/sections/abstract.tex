\begin{abstract}
The configuration in video analytics defines parameters including frame rate, image resolution, and model selection, and thus determines the inference accuracy and resource consumption. Traditional solutions to select a configuration are either fixed or periodically adjusted using a brute-force search scheme (i.e., periodically trying different configurations), and thus suffer either low inference accuracy or high computation cost. To this end, we propose a reinforcement learning-based configuration adaptation framework called AdaConfigure that dynamically selects configuration according to the spatial and temporal features of the current video stream. In particular, we use a video segmentation strategy to capture the characteristics of the video stream with much-reduced computation cost: profiling uses only 0.2-2\% computation resource. Our evaluation experiments on object detection task show that our approach outperforms baseline: it achieves 10-35\% higher accuracy with a similar amount of computation resources or achieves similar accuracy with only 10-50\% of the computation resource. 
\end{abstract}

%227 words
%The configuration in video analytics defines parameters including frame rate, image resolution, and model selection for video analytics pipeline, and thus determines the inference accuracy and resource consumption. Traditional solutions to select a configuration are either fixed (i.e., the same configuration is used all the time) or periodically adjusted using a brute-force search scheme (i.e., periodically trying different configurations and selecting the one with the best performance), and thus suffer either low inference accuracy or high computation cost to find a proper configuration timely. To this end, we propose a video analytical configuration adaptation framework called AdaConfigure that dynamically selects video configuration without resource-consuming exploration. First, we design a reinforcement learning-based framework in which an agent adaptively chooses the configuration according to the spatial and temporal features of the current video stream. In particular, we use a video segmentation strategy to capture the characteristics of the video stream with much-reduced computation cost: profiling uses only 0.2-2\% computation resource as compared to a full video. Second, we design a reward function that considers both the inference accuracy and computation resource consumption so that the configuration achieves good accuracy and resource consumption tradeoff. Our evaluation experiments on object detection task show that our approach outperforms baseline: it achieves 10-35\% higher accuracy with a similar amount of computation resources or achieves similar accuracy with only 10-50\% of the computation resource.

%zhaoliang
%Deep convolutional neural networks (NN)-based video analytics services demand intensive computation resources and high inference accuracy. Static configuration would either waste computation resources (by picking the highest frame rate and resolution) or decrease inference accuracy. Knowing this, adapting video configuration is still extremely challenging: 1) The best video configuration is determined by confounding factors, including the characteristics of the input video, the various accuracy-demand services, and computation resource, etc. 2) The cost of adaptive configuration profiling may far exceed the benefits of adaptive configurations. To tackle these challenges, we propose a reinforcement learning-based video analytics configuration framework, AdaConfigure. In particular, we design an agent that adaptively chooses the configuration according to the spatial and temporal features of video contexts. The design of reward carefully considers accuracy and computation resources to assess the impact of each configuration, guiding the agent learning to choose the best configuration for different accuracy-demand services. We leverage dividing video strategy and the extremely short choosing action time of agent to reduce profiling cost, which is only 0.2-2\% overhead to the overall video analytics services. Our evaluation experiments using the object detection task show that our approach outperforms static configurations by achieves 10-35\% higher accuracy with a similar amount of computation resources or achieves similar accuracy with only 60-90\% of the computation resources.

%We design a reward with comprehensive considering of accuracy and computation resources to assess each configuration's impact,

%2) It is hard to assess the impact that each configuration change will produce. 

%Deep convolutional neural networks (NN)-based video analytics services demand intensive computation resources and high inference accuracy. Due to the highly variable video context, the \emph{best} configuration also varies over time. If one chooses a static configuration (i.e., only profiles the video stream to choose the best configuration \emph{once}), the services would either waste computation resources (by picking an expensive configuration) or decrease inference accuracy. Conversely, searching for all configurations in an enormous space can lead to excessive resource overhead that far exceeds the benefits of periodic configurations. Knowing this, designing an \emph{adaptive} approach to choose the best configuration for the current video context is meaningful. We propose a reinforcement learning (RL)-based adaptive video analytics configuration framework, AutoConfigure. The unique feature of AutoConfigure is \emph{context-driven}, meaning that AutoConfigure can adapt the best configuration to intrusive dynamics of video contexts. In particular, our solution can choose the best configuration for the current video chunk according to the spatial and temporal features of video contexts. We implement and evaluate this approach in the object detection task, comparing its performance to static configuration. We show that AutoConfigure achieves 10-35\% higher accuracy with a similar amount of computation resources or achieves similar accuracy with only 60-90\% of the computation resources. Our solution proves to be more efficient than static solutions and only creates an overhead of 0.1-1\% to the overall video analytics services.
%and dynamic configuration baseline